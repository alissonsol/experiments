# AI, CUDA, and the Hardware Connection

Cloud providers are very good at providing you with compute options nowadays. Yet, you may have an "allocation problem" or cost surge exactly when you need that compute, mainly for applications that demand graphics processing units. GPUs have an enormous efficiency for AI, ML, crypto/blockchain processing, and games. That makes them costly, hard to buy, and then obsolete in just a couple of years. You shouldn't do capital expenditure for too many, but having a few at hand may be a safe bet!

Circa November/2021, among my hardware, I have a Dell [Alienware Aurora R11 Desktop](https://www.dell.com/en-us/shop/desktop-computers/alienware-aurora-r11-gaming-desktop/spd/alienware-aurora-r11-desktop) with a [GeForce RTX 3090](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090/) GPU. This is not even the "latest and greatest hardware" available. Dell already has R12, R13, and even R14 versions of the desktop. Meanwhile, the GPU market now has many new offerings, and I won't even go into the Apple M1 processors to avoid a more extended digression (at least not in this post).

But why the "efficiency"? Because you have libraries that enable applications to take action in multiple sections of arrays (matrices or "tensors", in a more generic way) in parallel. One of the most successful of such libraries is [CUDA](https://en.wikipedia.org/wiki/CUDA) (Compute Unified Device Architecture) from NVIDIA. And likely the best article to get a first glimpse at the power of CUDA is "[An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)" by Mark Harris. Another twist of the rapid progression of the hardware in this area is that it is hard to follow an article less than five years old. Even assuming you have the right hardware, drivers, and tools, you may hit some obstacles.

One of the ways to avoid polluting your machine with multiple versions of tools and then never knowing "what is executing" is to start containerized images in some platform like [Docker](https://docs.docker.com/get-docker/). I'm using Docker for Windows, with the [WSL-based engine](https://docs.docker.com/desktop/windows/wsl/). Let me try to go through the "Even Easier Introduction to CUDA" in that environment...

First, I needed to go to the [NVIDIA GPU Cloud (NGC) containers catalog](https://catalog.ngc.nvidia.com/containers) and get the pull tag for the CUDA container. At this time, the download command for the latest image is:

```shell
docker pull nvcr.io/nvidia/cuda:11.4.2-devel-ubuntu20.04
```

Now, let's start an interactive instance making GPUs available and check if everything is installed correctly:

```shell
docker run -it --gpus all nvcr.io/nvidia/cuda:11.4.2-devel-ubuntu20.04
```

From the container prompt (which is based on Linux), check the GPU access:

```shell
nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.47       Driver Version: 496.76       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |
| 31%   23C    P8    14W / 350W |   1692MiB / 24576MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

Before proceeding, a system update and some downloads are needed.

```shell
apt-get update
apt-get install -y wget vim git
```

Great! It is time to follow the guidance from "[An Even Easier Introduction to CUDA](https://developer.nvidia.com/blog/even-easier-introduction-cuda/)" by Mark Harris. Adapted code from the article is in the folder [2021-11.experiments.CUDA](./2021-11.experiments.CUDA). Commands below assume execution from that folder.

```shell
git clone https://github.com/alissonsol/experiments.git
cd experiments/2021/2021-11.experiments.CUDA
g++ add.cpp -o add

./add
N: 1000000
Max error: 0
```

Try to see the execution time for the application (`time ./add`) and then see how that changes as the vectors change, passing a parameter to change the default size (`time ./add 7`). That is the power of 10 that will be used to size the vector.

Now, let's try the CUDA-based version of the code.

```shell
nvcc add.cu -o add_cuda

./add_cuda
N: 1000000
Max error: 0
```

So far, everything is working. Let's then try profiling as per instructions in the original article.

```shell
nvprof ./add_cuda
======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.
                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.
                  Refer https://developer.nvidia.com/tools-overview for more details.
```

Oops! Despite my GPU not being the latest and greatest, it is not supported by the `nvprof` tool. I have to download the Nsight Systems tool. That is another battle by itself. I could download it and copy it to the container. But I wanted to learn how to download it from inside the container (should that be needed for later test automation!).

I tried to download the file directly from the download center with `wget`. Yet, that didn't work. I needed to learn via [support forums](https://forums.developer.nvidia.com/t/download-cudnn-via-wget-or-curl/48952/4) how to get to the real URL for the download. That demanded installing a [Link Redirect Trace](https://chrome.google.com/webstore/detail/link-redirect-trace/nnpljppamoaalgkieeciijbcccohlpoh) to get what is a temporarily valid URL for the logged-in user in the download center (so, basically, I didn't solve my automation problem, and likely would need to create an image already with the downloaded tools!).

Nevertheless, at least in my "temporary valid scenario", I could proceed.

```shell
wget -O install.run --user [user] --password [password] https://developer.download.nvidia.com/assets/tools/secure/nsight-systems/2021_5/NsightSystems-linux-public-2021.5.1.77-4a17e7d.run?yN6XISf4ZE49F5cK9U87mp8hxJ92vbJTsUUyF-NVdK2AtS7Ar6o-poh0o5E2H-mEdtmRJtt1rvyM9lJ8aEeEP1H0VPYAi_ubDIBOlA0Ie42Q6GFmlrIZB9qW7kGTWaq-2MKJtIaHXkTR4TZin-ZuWVs9JvdpIs76YTkLug-ZSEXUhDa3I9wqmgGJJ_jh4PPJNmUe6BErIQ33170&t=eyJscyI6IndlYnNpdGUiLCJsc2QiOiJkZXZlbG9wZXIubnZpZGlhLmNvbVwvdG9vbHMtb3ZlcnZpZXcifQ%3D%3D
```

**NOTE**: the URL cookie above is just provided as an example. You have to replace it with your own, valid for a limited time. Before proceeding, check that files `wget-log` and `install.run` indicate a succesful download. In case of failure, the file `install.run` will be a few hundred bytes large and just contain some HTML. As of 2021-11, it has 283,555,268 bytes on successful download and it is a shell script (starts with `#!/bin/sh`).

```shell
chmod a+x install.run
./install.run
```

By default, it will install in a folder like: `/opt/nvidia/nsight-systems/2021.5.1`.
You will need to add the `bin` folder under that to your path (and likely to `.bashrc`), using a command like: `export PATH="/opt/nvidia/nsight-systems/2021.5.1/bin:$PATH"`

Now, one can use `nsys` commands to profile the CUDA-based version of the article code. See more detailed instructions in the NVIDIA blog post about "[Migrating to NVIDIA Nsight Tools from NVVP and Nvprof](https://developer.nvidia.com/blog/migrating-nvidia-nsight-tools-nvvp-nvprof/)".

```shell
nsys nvprof ./add_cuda
WARNING: add_cuda and any of its children processes will be profiled.

N: 1000000
Max error: 0
...
CUDA API Statistics:

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------
     81.7        778513900          1  778513900.0  778513900.0  778513900  778513900          0.0  cudaDeviceSynchronize
     18.1        172996500          2   86498250.0   86498250.0    2534800  170461700  118742249.7  cudaMallocManaged
      0.2          1890900          2     945450.0     945450.0     845800    1045100     140926.4  cudaFree
      0.0            51200          1      51200.0      51200.0      51200      51200          0.0  cudaLaunchKernel
```

Those numbers will change each time you execute the application. Yet, they indicate that the total execution time is ~950ms (almost a second). There are other version of the application in the folder as per the article. One to add blocks (`add_block.cu`) and another for the final grid-based (`add_grid.cu`).

```shell
nvcc add_block.cu -o add_block_cuda
nsys nvprof ./add_block_cuda
WARNING: add_block_cuda and any of its children processes will be profiled.

N: 1000000
Max error: 0
...
CUDA API Statistics:

Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name
 --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------
     97.1        197427900          2  98713950.0  98713950.0   2973800  194454100  135397018.6  cudaMallocManaged
      1.9          3961600          1   3961600.0   3961600.0   3961600    3961600          0.0  cudaDeviceSynchronize
      1.0          1971300          2    985650.0    985650.0    912200    1059100     103874.0  cudaFree
      0.0            59100          1     59100.0     59100.0     59100      59100          0.0  cudaLaunchKernel
```

Similarly, compiling and profiling the grid-based version outputs something similar to what is copied below.

```shell
nvcc add_grid.cu -o add_grid_cuda
nsys nvprof ./add_grid_cuda
...
CUDA API Statistics:

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name
 --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------
     98.0        181801900          2  90900950.0  90900950.0   2545100  179256800  124954041.4  cudaMallocManaged
      1.1          2005100          2   1002550.0   1002550.0    932900    1072200      98500.0  cudaFree
      0.9          1707500          1   1707500.0   1707500.0   1707500    1707500          0.0  cudaDeviceSynchronize
      0.0            60100          1     60100.0     60100.0     60100      60100          0.0  cudaLaunchKernel
```


In my machine, I could execute the different version of the application with a different size (`N`) and produce the raw data table below.

```shell
         add    add_cuda    add_block_cuda    add_grid_cuda
N6     0.016        0.96         0.23           0.23
N7     0.15         7.82         0.41           0.37
N8     1.42        76.9          2.01           1.83
N9    14.4
```

Times are in seconds. I did at least 3 measurements for each combination of version and input size, and averaged the times. I also couldn't run the cuda-based version for `10^9` due to a segmentation fault (which happens during memory allocation). Read the Documentation on how to [Configure Linux distributions](https://docs.microsoft.com/en-us//windows/wsl/wsl-config) and created a `%UserProfile%/.wslconfig` file with the following content.

```shell
[wsl2]
memory=36GB  # Any size you feel like
localhostForwarding=true
processors=16
```

Stopping WSL (`wsl --shutdown`) brings up a pop-up asking to restart Docker. After the restart, update and reinstall of requirements, there is still the memory allocation issue in the CUDA unified memory, which is being investigated. Nevertheless, using the GPU clearly pays off as the problem size grows. Profile the versions and check where time is being used!

The Alisson Sol [experiments](https://github.com/alissonsol/experiments) - Circa 2021/11
